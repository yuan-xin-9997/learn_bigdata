package org.atguigu.sparkstreaming.demos

import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

/*
* 无状态运算：每个批次之间是相互独立的，各算各的
  有状态运算：当前批次的计算需要使用到上一个批次运算的结果。在上一个运算的基础上继续运算。（无需要掌握）。因为SparkStreaming
  *         原生的计算方式有很大的弊端，
*           虽然提供了有状态计算的算子，但是没有人用。DStream.xxxStatexxx() SparkStreaming提供的有状态计算算子。
*           后续使用其他的方式解决这个问题。
*
* ------------------------------------------
* select
*   a, xxx(b)  ---UDF（输入1行，输出1行）map
* from xxx
* where xxx    ---filter
* group by     --- reduceByKey | groupByKey
* order by     --- sortBy | sortByKey
* limit x      --- take()
*
*
* ------------------------------------------
* transform:
   Return a new DStream in which each RDD is generated by applying a function on each RDD of 'this' DStream.
   def transform[U: ClassTag](transformFunc: RDD[T] => RDD[U]): DStream[U] = ssc.withScope {
   功能：将DStream[T]中的RDD[T]取出来，调用transformFunc，转换为RDD[U]，封装为DStream[U]
*
*  算子：RDD.map, RDD.filter
*  抽象原语：DStream.map, DStream.filter
*
*  算子远比抽象原语丰富，有些方法算子有，但是抽象原语没有。如果希望使用此类方法，只能将DStream的运算转换为对其中RDD的运算，才能调用RDD的算子
*  算完后再把结果封装为DStream
* */

object TransformDemo {
  def main(args: Array[String]): Unit = {
    // 创建 streamingContext 方式3
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("transformDemo").set("spark.testing.memory", "2147480000")
    val sparkContext = new SparkContext(sparkConf)
    val streamingContext = new StreamingContext(sparkContext, Seconds(10))

    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "hadoop102:9092,hadoop103:9092",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "20240506",
      "auto.offset.reset" -> "latest",
      "enable.auto.commit" -> (true: java.lang.Boolean)
    )

    val topics = Array("topicA")

    val ds: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](
      streamingContext,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams)
    )

    // (String, Int):(word, n)
    val ds1: DStream[(String, Int)] = ds.flatMap(record => record.value().split(" "))
      .map((_, 1)).reduceByKey(_+_)

    // 对单词进行sortByKey操作
//    ds1.sortByKey  //抽象原语没有此算子。RDD里面有

    val ds2: DStream[(String, Int)] = ds1.transform(rdd => rdd.sortByKey())

    // 输出：在屏幕打印
    //   print() 默认打印10行
    ds2.print(1000)

    // 启动APP
    streamingContext.start()

    // 阻塞进程，让进程一直运行
    streamingContext.awaitTermination()
  }
}
