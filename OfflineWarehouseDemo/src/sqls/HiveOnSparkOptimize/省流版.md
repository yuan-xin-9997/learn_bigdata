## 1.Hive官方建议的Hive On Spark优化

```
mapreduce.input.fileinputformat.split.maxsize=750000000
hive.vectorized.execution.enabled=true

hive.cbo.enable=true
hive.optimize.reducededuplication.min.reducer=4
hive.optimize.reducededuplication=true
hive.orc.splits.include.file.footer=false
hive.merge.mapfiles=true
hive.merge.sparkfiles=false
hive.merge.smallfiles.avgsize=16000000
hive.merge.size.per.task=256000000
hive.merge.orcfile.stripe.level=true
hive.auto.convert.join=true
hive.auto.convert.join.noconditionaltask=true
hive.auto.convert.join.noconditionaltask.size=894435328
hive.optimize.bucketmapjoin.sortedmerge=false
hive.map.aggr.hash.percentmemory=0.5
hive.map.aggr=true
hive.optimize.sort.dynamic.partition=false
hive.stats.autogather=true
hive.stats.fetch.column.stats=true
hive.vectorized.execution.reduce.enabled=false
hive.vectorized.groupby.checkinterval=4096
hive.vectorized.groupby.flush.percent=0.1
hive.compute.query.using.stats=true
hive.limit.pushdown.memory.usage=0.4
hive.optimize.index.filter=true
hive.exec.reducers.bytes.per.reducer=67108864
hive.smbjoin.cache.rows=10000
hive.exec.orc.default.stripe.size=67108864
hive.fetch.task.conversion=more
hive.fetch.task.conversion.threshold=1073741824
hive.fetch.task.aggr=false
mapreduce.input.fileinputformat.list-status.num-threads=5
spark.kryo.referenceTracking=false
spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch
```

CDH 建议的hive on spark优化

```
https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hos_tuning.html#hos_tuning
```



## 2.集群规划

HA： HDFS,YARN

2台，3台节点 ，运行master

其他节点运行 Datanode , Nodemanger 。

Kafka 一般选择 拥有SSD的节点。



中小公司 10 - 20 足够。



## 3.YARN配置

单个Nodemanger贡献给YARN的core,mem

```
在Nodemanger节点，配置yarn-site.xml

yarn.nodemanager.resource.memory-mb

yarn.nodemanager.resource.cpu-vcores
```

YARN的资源总和等于所有NM提供的core和mem的总和。



## 4.Container资源的上下限

```
yarn.scheduler.maximum-allocation-mb
yarn.scheduler.minimum-allocation-mb

yarn.scheduler.minimum-allocation-cpu-vcores
yarn.scheduler.maximum-allocation-cpu-vcores
```

需要参考集群中所运行的最大App需要的上下限。

配置上下限后，所有提交的APP所申请的资源，不能超过Container的上下限，超过，提交失败。



## 5.Executor的参数

CPU:  每个Executor运行的容器中，配置core >= 4。 如果希望YARN的资源充分利用(没有空闲)， CPU设置为 集群总CPU数的因子(约数)



Executor内存：  业界经验，CPU : MEM = 1：4 。

Driver内存：  

```
yarn.nodemanager.resource.memory-mb设置为X，
若X>50G，则Driver可设置为12G，
若12G<X<50G，则Driver可设置为4G。
若1G<X<12G，则Driver可设置为1G。 
```



Spark的内存模型，每个容器中的内存，不管是Executor还是Driver，都要默认使用 10%作为预留内存。

​			配置预留内存 :    ceil( 容器中申请的总的内存数 / 10 )  

​							spark.executor.memoryOverhead

​                            spark.yarn.driver.memoryOverhead

​			配置非预留内存： 容器中申请的总的内存数 -   ceil( 容器中申请的总的内存数 / 10 ) 

​							spark.driver.memory

​							spark.executor.memory



如果希望只是影响Hive On Spark，在Hive的conf目录中的 spark-defaults.conf中配置

如果希望影响所有的SparkApp，在Spark的conf目录中的 spark-defaults.conf中配置

## 6.Executor的个数

静态配置：  指定起多少个

```
--num-executors n

--conf spark.executor.instances n
```



动态配置:

具体参考3.3

```
spark.dynamicAllocation.enabled  true
spark.shuffle.service.enabled  true
spark.dynamicAllocation.executorIdleTimeout  60s
spark.dynamicAllocation.initialExecutors    1
spark.dynamicAllocation.minExecutors  1
spark.dynamicAllocation.maxExecutors  11
spark.dynamicAllocation.schedulerBacklogTimeout 1s
spark.shuffle.useOldFetchProtocol    true
....
```

## 7.group by优化

什么都不用干，默认开启了map端聚合

```
set hive.map.aggr=true;
```

## 8.Join优化

map join 仅仅适用于 大表  join 小表

大表  join  大表 ：  表不是分桶表，只能走 common(reduce) join
					如果都是分桶表，分桶的字段就是join的字段，走 SMB join
					
大表  join  小表 ： 表不是分桶表，能走 map join
					是分桶表，分桶的字段就是join的字段， 走 bucket map join（特殊情形的map join）



```
--启用map join自动转换
set hive.auto.convert.join=true;
--common join转map join小表阈值
set hive.auto.convert.join.noconditionaltask.size = xxxx 参考小表的加入到内存中大小  rawDataSize
```



map join涵盖数仓中99%的场景。

数仓中基于维度建模创建表，最经典的是 星型模型(99%的业务场景)



事实表  join  维度表

## 9.数据倾斜优化

小文件和文件小是两回事。

1B的文件：  如果数据总量就是1B ， 就是文件小。

​					如果数据总量是1TB， 就是小文件。



本质：  在shuffle后，大key被分导了一个ReduceTask,造成这个ReduceTask运行时间远远大于其他的ReduceTask，拖累全局。



第一种： 避免大key被shuffle。

​				16行  是key 大

​				如果group by场景，开启Map端聚合



第二种：  避免shuffle

​					join时倾斜，直接使用MapJoin



第三种：  开启skewjoin(hive默认不推荐，效果不明显)



## 10.并行度

默认使用  CombineHiveInputFormat把输入目录中的所有文件合并成一个整体，以整体为大小去切片。

不修改，防止输入的小文件过多。

能调整:

```
set mapreduce.input.fileinputformat.split.maxsize=256M;

希望调大并行度，设置以上值变小。反之，调小。
```





reduce端：

```
熟悉数据，自己设置

set mapreduce.job.reduces=n;

不熟悉数据，让hive推测(基于准确的统计信息去推测):
set mapreduce.job.reduces=-1;
--执行DML语句时，收集表级别的统计信息
set hive.stats.autogather=true;
--执行DML语句时，收集字段级别的统计信息
set hive.stats.column.autogather=true;
--计算Reduce并行度时，从上游Operator统计信息获得输入数据量
set hive.spark.use.op.stats=true;
--计算Reduce并行度时，使用列级别的统计信息估算输入数据量
set hive.stats.fetch.column.stats=true;
```

## 11.小文件优化

```
输出合并小文件:
set hive.merge.sparkfiles=true;
```

## 12.默认

```
矢量化，只有对ORC类型的文件才有效
```

